{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This Notebook aims to evaluate cross validation result obtained using the train_models_crossval notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, the predictive performance is considered using the AUC performance metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import standard modules for data handling and visualization\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "## import model specific modules\n",
    "import cplex as cp\n",
    "import slim_python as slim\n",
    "import shap\n",
    "from interpret.glassbox import ExplainableBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danie\\Documents\\StageDaniel\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danie\\Documents\\StageDaniel\\research\n"
     ]
    }
   ],
   "source": [
    "cd research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models(name):\n",
    "    with open('results/models/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def extract_results(results, X):\n",
    "    # Extract specific models from the results\n",
    "\n",
    "    slim_results = results[0][1]\n",
    "\n",
    "    rho = slim_results['rho']\n",
    "    slim_predictions = pred_slim(X,rho)\n",
    "\n",
    "    ebm = results[1]\n",
    "\n",
    "    XGboost = results[2]\n",
    "    explainer = shap.TreeExplainer(XGboost)\n",
    "    shap_values = explainer.shap_values(X)\n",
    "\n",
    "    logit = results[3]\n",
    "    \n",
    "    return rho, slim_predictions, ebm, explainer, shap_values, logit\n",
    "\n",
    "## Simple function for getting predictions for a SLIM scoring system\n",
    "def pred_slim(X, rho):\n",
    "    return (X.dot(rho[1:])+rho[0]>=0)*1\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def scale_sigmoid(x):\n",
    "    x_max = np.maximum(x.max(), np.abs(x.min()))\n",
    "    x = x/(x_max/5)\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def auc(y, y_pred):\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y, y_pred)\n",
    "    return metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_measures(data, models):\n",
    "    train_aucs = []\n",
    "    test_aucs = []\n",
    "    \n",
    "    X_train = data['X_train']\n",
    "    X_test = data['X_test']\n",
    "    y_train = data['y_train']\n",
    "    y_test = data['y_test']\n",
    "    \n",
    "    slim_results = models[0][1]\n",
    "    rho = slim_results['rho']\n",
    "    train_pred = scale_sigmoid(X_train.dot(rho[1:])+rho[0])\n",
    "    test_pred = sigmoid(X_test.dot(rho[1:])+rho[0])\n",
    "    train_aucs.append(auc(y_train,train_pred))\n",
    "    test_aucs.append(auc(y_test,test_pred))\n",
    "    \n",
    "    ebm = models[1]\n",
    "    train_pred = ebm.predict_proba(X_train)\n",
    "    test_pred = ebm.predict_proba(X_test)\n",
    "    train_aucs.append(auc(y_train,train_pred[:,1]))\n",
    "    test_aucs.append(auc(y_test,test_pred[:,1]))\n",
    "    \n",
    "    XGBoost = models[2]\n",
    "    train_pred = XGBoost.predict_proba(X_train)\n",
    "    test_pred = XGBoost.predict_proba(X_test)\n",
    "    train_aucs.append(auc(y_train,train_pred[:,1]))\n",
    "    test_aucs.append(auc(y_test,test_pred[:,1]))\n",
    "    \n",
    "    logit = models[3]\n",
    "    train_pred = logit.predict_proba(X_train)\n",
    "    test_pred = logit.predict_proba(X_test)\n",
    "    train_aucs.append(auc(y_train,train_pred[:,1]))\n",
    "    test_aucs.append(auc(y_test,test_pred[:,1]))\n",
    "\n",
    "    return pd.DataFrame([train_aucs, test_aucs], ['train', 'test'], ['slim', 'ebm', 'shap', 'logit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>slim</th>\n",
       "      <th>ebm</th>\n",
       "      <th>shap</th>\n",
       "      <th>logit</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>validation</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>test</td>\n",
       "      <td>0.847226</td>\n",
       "      <td>0.890406</td>\n",
       "      <td>0.893390</td>\n",
       "      <td>0.890600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>train</td>\n",
       "      <td>0.846707</td>\n",
       "      <td>0.891326</td>\n",
       "      <td>0.906507</td>\n",
       "      <td>0.891406</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                slim       ebm      shap     logit\n",
       "validation                                        \n",
       "test        0.847226  0.890406  0.893390  0.890600\n",
       "train       0.846707  0.891326  0.906507  0.891406"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = ['bankruptcy','breastcancer','haberman','heart','mammo','mushroom','spambase', 'adult']\n",
    "performances = []\n",
    "sds = []\n",
    "\n",
    "for dataname in names:\n",
    "    fold_performances = []\n",
    "    for fold in range(5):\n",
    "        models = load_models(dataname+'_models_600_cv'+str(fold))\n",
    "        data = load_models(dataname+'_data_cv'+str(fold))\n",
    "        performance = performance_measures(data, models)\n",
    "        fold_performances.append(performance)\n",
    "    folds_df = pd.concat(fold_performances)\n",
    "    folds_df.index.names = ['validation']\n",
    "    performances.append(folds_df.groupby('validation').mean())\n",
    "    sds.append(folds_df.groupby('validation').std())\n",
    "performances[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>slim</th>\n",
       "      <th>ebm</th>\n",
       "      <th>shap</th>\n",
       "      <th>logit</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>validation</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>test</td>\n",
       "      <td>0.011512</td>\n",
       "      <td>0.004509</td>\n",
       "      <td>0.005023</td>\n",
       "      <td>0.004478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>train</td>\n",
       "      <td>0.006373</td>\n",
       "      <td>0.001113</td>\n",
       "      <td>0.002918</td>\n",
       "      <td>0.001153</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                slim       ebm      shap     logit\n",
       "validation                                        \n",
       "test        0.011512  0.004509  0.005023  0.004478\n",
       "train       0.006373  0.001113  0.002918  0.001153"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sds[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>validation</th>\n",
       "      <th>slim</th>\n",
       "      <th>ebm</th>\n",
       "      <th>shap</th>\n",
       "      <th>logit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>bankruptcy</td>\n",
       "      <td>test</td>\n",
       "      <td>0.006518</td>\n",
       "      <td>0.000761</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>bankruptcy</td>\n",
       "      <td>train</td>\n",
       "      <td>0.002853</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>breastcancer</td>\n",
       "      <td>test</td>\n",
       "      <td>0.005987</td>\n",
       "      <td>0.007320</td>\n",
       "      <td>8.019168e-03</td>\n",
       "      <td>4.570280e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>breastcancer</td>\n",
       "      <td>train</td>\n",
       "      <td>0.003115</td>\n",
       "      <td>0.000734</td>\n",
       "      <td>4.987587e-05</td>\n",
       "      <td>1.350922e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>haberman</td>\n",
       "      <td>test</td>\n",
       "      <td>0.107018</td>\n",
       "      <td>0.080087</td>\n",
       "      <td>1.312392e-01</td>\n",
       "      <td>1.101864e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>haberman</td>\n",
       "      <td>train</td>\n",
       "      <td>0.028860</td>\n",
       "      <td>0.025989</td>\n",
       "      <td>1.839336e-02</td>\n",
       "      <td>1.753060e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>heart</td>\n",
       "      <td>test</td>\n",
       "      <td>0.023146</td>\n",
       "      <td>0.031294</td>\n",
       "      <td>3.807233e-02</td>\n",
       "      <td>2.890056e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>heart</td>\n",
       "      <td>train</td>\n",
       "      <td>0.009499</td>\n",
       "      <td>0.020579</td>\n",
       "      <td>2.329821e-04</td>\n",
       "      <td>8.636592e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>mammo</td>\n",
       "      <td>test</td>\n",
       "      <td>0.039978</td>\n",
       "      <td>0.025840</td>\n",
       "      <td>1.743885e-02</td>\n",
       "      <td>2.419397e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>mammo</td>\n",
       "      <td>train</td>\n",
       "      <td>0.017947</td>\n",
       "      <td>0.006060</td>\n",
       "      <td>4.052506e-03</td>\n",
       "      <td>5.525324e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>mushroom</td>\n",
       "      <td>test</td>\n",
       "      <td>0.117686</td>\n",
       "      <td>0.065525</td>\n",
       "      <td>5.215770e-03</td>\n",
       "      <td>1.303943e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>mushroom</td>\n",
       "      <td>train</td>\n",
       "      <td>0.005918</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.551115e-17</td>\n",
       "      <td>5.551115e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>spambase</td>\n",
       "      <td>test</td>\n",
       "      <td>0.063370</td>\n",
       "      <td>0.258993</td>\n",
       "      <td>3.128826e-02</td>\n",
       "      <td>4.275751e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>spambase</td>\n",
       "      <td>train</td>\n",
       "      <td>0.023970</td>\n",
       "      <td>0.268406</td>\n",
       "      <td>2.681355e-04</td>\n",
       "      <td>4.733234e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>adult</td>\n",
       "      <td>test</td>\n",
       "      <td>0.011512</td>\n",
       "      <td>0.004509</td>\n",
       "      <td>5.022754e-03</td>\n",
       "      <td>4.478194e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>adult</td>\n",
       "      <td>train</td>\n",
       "      <td>0.006373</td>\n",
       "      <td>0.001113</td>\n",
       "      <td>2.918443e-03</td>\n",
       "      <td>1.153284e-03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         dataset validation      slim       ebm          shap         logit\n",
       "0     bankruptcy       test  0.006518  0.000761  0.000000e+00  0.000000e+00\n",
       "1     bankruptcy      train  0.002853  0.000000  0.000000e+00  0.000000e+00\n",
       "2   breastcancer       test  0.005987  0.007320  8.019168e-03  4.570280e-03\n",
       "3   breastcancer      train  0.003115  0.000734  4.987587e-05  1.350922e-03\n",
       "4       haberman       test  0.107018  0.080087  1.312392e-01  1.101864e-01\n",
       "5       haberman      train  0.028860  0.025989  1.839336e-02  1.753060e-02\n",
       "6          heart       test  0.023146  0.031294  3.807233e-02  2.890056e-02\n",
       "7          heart      train  0.009499  0.020579  2.329821e-04  8.636592e-03\n",
       "8          mammo       test  0.039978  0.025840  1.743885e-02  2.419397e-02\n",
       "9          mammo      train  0.017947  0.006060  4.052506e-03  5.525324e-03\n",
       "10      mushroom       test  0.117686  0.065525  5.215770e-03  1.303943e-04\n",
       "11      mushroom      train  0.005918  0.000000  5.551115e-17  5.551115e-17\n",
       "12      spambase       test  0.063370  0.258993  3.128826e-02  4.275751e-02\n",
       "13      spambase      train  0.023970  0.268406  2.681355e-04  4.733234e-03\n",
       "14         adult       test  0.011512  0.004509  5.022754e-03  4.478194e-03\n",
       "15         adult      train  0.006373  0.001113  2.918443e-03  1.153284e-03"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# corrs_df = pd.concat(corrs, keys = names)\n",
    "sds_df = pd.concat(sds, keys= names)\n",
    "sds_df.index.names = ['dataset', 'validation']\n",
    "sds_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>validation</th>\n",
       "      <th>slim</th>\n",
       "      <th>ebm</th>\n",
       "      <th>shap</th>\n",
       "      <th>logit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>bankruptcy</td>\n",
       "      <td>test</td>\n",
       "      <td>0.992971</td>\n",
       "      <td>0.999660</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>bankruptcy</td>\n",
       "      <td>train</td>\n",
       "      <td>0.996773</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>breastcancer</td>\n",
       "      <td>test</td>\n",
       "      <td>0.985562</td>\n",
       "      <td>0.992550</td>\n",
       "      <td>0.992071</td>\n",
       "      <td>0.995222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>breastcancer</td>\n",
       "      <td>train</td>\n",
       "      <td>0.991098</td>\n",
       "      <td>0.998785</td>\n",
       "      <td>0.999912</td>\n",
       "      <td>0.996353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>haberman</td>\n",
       "      <td>test</td>\n",
       "      <td>0.730686</td>\n",
       "      <td>0.629926</td>\n",
       "      <td>0.631642</td>\n",
       "      <td>0.673922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>haberman</td>\n",
       "      <td>train</td>\n",
       "      <td>0.688255</td>\n",
       "      <td>0.823047</td>\n",
       "      <td>0.929665</td>\n",
       "      <td>0.704827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>heart</td>\n",
       "      <td>test</td>\n",
       "      <td>0.886813</td>\n",
       "      <td>0.908186</td>\n",
       "      <td>0.900541</td>\n",
       "      <td>0.898936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>heart</td>\n",
       "      <td>train</td>\n",
       "      <td>0.894270</td>\n",
       "      <td>0.954389</td>\n",
       "      <td>0.999027</td>\n",
       "      <td>0.936305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>mammo</td>\n",
       "      <td>test</td>\n",
       "      <td>0.798674</td>\n",
       "      <td>0.850296</td>\n",
       "      <td>0.844936</td>\n",
       "      <td>0.855403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>mammo</td>\n",
       "      <td>train</td>\n",
       "      <td>0.810708</td>\n",
       "      <td>0.859919</td>\n",
       "      <td>0.874816</td>\n",
       "      <td>0.860082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>mushroom</td>\n",
       "      <td>test</td>\n",
       "      <td>0.929572</td>\n",
       "      <td>0.970629</td>\n",
       "      <td>0.997667</td>\n",
       "      <td>0.999942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>mushroom</td>\n",
       "      <td>train</td>\n",
       "      <td>0.989913</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>spambase</td>\n",
       "      <td>test</td>\n",
       "      <td>0.897987</td>\n",
       "      <td>0.692424</td>\n",
       "      <td>0.974185</td>\n",
       "      <td>0.957890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>spambase</td>\n",
       "      <td>train</td>\n",
       "      <td>0.914232</td>\n",
       "      <td>0.698764</td>\n",
       "      <td>0.999248</td>\n",
       "      <td>0.977765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>adult</td>\n",
       "      <td>test</td>\n",
       "      <td>0.847226</td>\n",
       "      <td>0.890406</td>\n",
       "      <td>0.893390</td>\n",
       "      <td>0.890600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>adult</td>\n",
       "      <td>train</td>\n",
       "      <td>0.846707</td>\n",
       "      <td>0.891326</td>\n",
       "      <td>0.906507</td>\n",
       "      <td>0.891406</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         dataset validation      slim       ebm      shap     logit\n",
       "0     bankruptcy       test  0.992971  0.999660  1.000000  1.000000\n",
       "1     bankruptcy      train  0.996773  1.000000  1.000000  1.000000\n",
       "2   breastcancer       test  0.985562  0.992550  0.992071  0.995222\n",
       "3   breastcancer      train  0.991098  0.998785  0.999912  0.996353\n",
       "4       haberman       test  0.730686  0.629926  0.631642  0.673922\n",
       "5       haberman      train  0.688255  0.823047  0.929665  0.704827\n",
       "6          heart       test  0.886813  0.908186  0.900541  0.898936\n",
       "7          heart      train  0.894270  0.954389  0.999027  0.936305\n",
       "8          mammo       test  0.798674  0.850296  0.844936  0.855403\n",
       "9          mammo      train  0.810708  0.859919  0.874816  0.860082\n",
       "10      mushroom       test  0.929572  0.970629  0.997667  0.999942\n",
       "11      mushroom      train  0.989913  1.000000  1.000000  1.000000\n",
       "12      spambase       test  0.897987  0.692424  0.974185  0.957890\n",
       "13      spambase      train  0.914232  0.698764  0.999248  0.977765\n",
       "14         adult       test  0.847226  0.890406  0.893390  0.890600\n",
       "15         adult      train  0.846707  0.891326  0.906507  0.891406"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# corrs_df = pd.concat(corrs, keys = names)\n",
    "performance_df = pd.concat(performances, keys= names)\n",
    "performance_df.index.names = ['dataset', 'validation']\n",
    "performance_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df = pd.concat([performance_df, sds_df], keys = ['AUC','sdev'])\n",
    "total_df.index.names = ['metric','dataset', 'validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>slim</th>\n",
       "      <th>ebm</th>\n",
       "      <th>shap</th>\n",
       "      <th>logit</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>metric</th>\n",
       "      <th>dataset</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td rowspan=\"8\" valign=\"top\">AUC</td>\n",
       "      <td>bankruptcy</td>\n",
       "      <td>0.992971</td>\n",
       "      <td>0.999660</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>breastcancer</td>\n",
       "      <td>0.985562</td>\n",
       "      <td>0.992550</td>\n",
       "      <td>0.992071</td>\n",
       "      <td>0.995222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>haberman</td>\n",
       "      <td>0.730686</td>\n",
       "      <td>0.629926</td>\n",
       "      <td>0.631642</td>\n",
       "      <td>0.673922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>heart</td>\n",
       "      <td>0.886813</td>\n",
       "      <td>0.908186</td>\n",
       "      <td>0.900541</td>\n",
       "      <td>0.898936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mammo</td>\n",
       "      <td>0.798674</td>\n",
       "      <td>0.850296</td>\n",
       "      <td>0.844936</td>\n",
       "      <td>0.855403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mushroom</td>\n",
       "      <td>0.929572</td>\n",
       "      <td>0.970629</td>\n",
       "      <td>0.997667</td>\n",
       "      <td>0.999942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>spambase</td>\n",
       "      <td>0.897987</td>\n",
       "      <td>0.692424</td>\n",
       "      <td>0.974185</td>\n",
       "      <td>0.957890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>adult</td>\n",
       "      <td>0.847226</td>\n",
       "      <td>0.890406</td>\n",
       "      <td>0.893390</td>\n",
       "      <td>0.890600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td rowspan=\"8\" valign=\"top\">sdev</td>\n",
       "      <td>bankruptcy</td>\n",
       "      <td>0.006518</td>\n",
       "      <td>0.000761</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>breastcancer</td>\n",
       "      <td>0.005987</td>\n",
       "      <td>0.007320</td>\n",
       "      <td>0.008019</td>\n",
       "      <td>0.004570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>haberman</td>\n",
       "      <td>0.107018</td>\n",
       "      <td>0.080087</td>\n",
       "      <td>0.131239</td>\n",
       "      <td>0.110186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>heart</td>\n",
       "      <td>0.023146</td>\n",
       "      <td>0.031294</td>\n",
       "      <td>0.038072</td>\n",
       "      <td>0.028901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mammo</td>\n",
       "      <td>0.039978</td>\n",
       "      <td>0.025840</td>\n",
       "      <td>0.017439</td>\n",
       "      <td>0.024194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mushroom</td>\n",
       "      <td>0.117686</td>\n",
       "      <td>0.065525</td>\n",
       "      <td>0.005216</td>\n",
       "      <td>0.000130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>spambase</td>\n",
       "      <td>0.063370</td>\n",
       "      <td>0.258993</td>\n",
       "      <td>0.031288</td>\n",
       "      <td>0.042758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>adult</td>\n",
       "      <td>0.011512</td>\n",
       "      <td>0.004509</td>\n",
       "      <td>0.005023</td>\n",
       "      <td>0.004478</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         slim       ebm      shap     logit\n",
       "metric dataset                                             \n",
       "AUC    bankruptcy    0.992971  0.999660  1.000000  1.000000\n",
       "       breastcancer  0.985562  0.992550  0.992071  0.995222\n",
       "       haberman      0.730686  0.629926  0.631642  0.673922\n",
       "       heart         0.886813  0.908186  0.900541  0.898936\n",
       "       mammo         0.798674  0.850296  0.844936  0.855403\n",
       "       mushroom      0.929572  0.970629  0.997667  0.999942\n",
       "       spambase      0.897987  0.692424  0.974185  0.957890\n",
       "       adult         0.847226  0.890406  0.893390  0.890600\n",
       "sdev   bankruptcy    0.006518  0.000761  0.000000  0.000000\n",
       "       breastcancer  0.005987  0.007320  0.008019  0.004570\n",
       "       haberman      0.107018  0.080087  0.131239  0.110186\n",
       "       heart         0.023146  0.031294  0.038072  0.028901\n",
       "       mammo         0.039978  0.025840  0.017439  0.024194\n",
       "       mushroom      0.117686  0.065525  0.005216  0.000130\n",
       "       spambase      0.063370  0.258993  0.031288  0.042758\n",
       "       adult         0.011512  0.004509  0.005023  0.004478"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_df.xs('test', level=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrrrrr}\n",
      "\\toprule\n",
      "{} & \\multicolumn{8}{l}{0} \\\\\n",
      "model & \\multicolumn{2}{l}{ebm} & \\multicolumn{2}{l}{logit} & \\multicolumn{2}{l}{shap} & \\multicolumn{2}{l}{slim} \\\\\n",
      "metric &    AUC &   sdev &    AUC &   sdev &    AUC &   sdev &    AUC &   sdev \\\\\n",
      "dataset      &        &        &        &        &        &        &        &        \\\\\n",
      "\\midrule\n",
      "bankruptcy   &  1.000 &  0.000 &  1.000 &  0.000 &  1.000 &  0.000 &  0.997 &  0.003 \\\\\n",
      "breastcancer &  0.999 &  0.001 &  0.996 &  0.001 &  1.000 &  0.000 &  0.991 &  0.003 \\\\\n",
      "haberman     &  0.823 &  0.026 &  0.705 &  0.018 &  0.930 &  0.018 &  0.688 &  0.029 \\\\\n",
      "heart        &  0.954 &  0.021 &  0.936 &  0.009 &  0.999 &  0.000 &  0.894 &  0.009 \\\\\n",
      "mammo        &  0.860 &  0.006 &  0.860 &  0.006 &  0.875 &  0.004 &  0.811 &  0.018 \\\\\n",
      "mushroom     &  1.000 &  0.000 &  1.000 &  0.000 &  1.000 &  0.000 &  0.990 &  0.006 \\\\\n",
      "spambase     &  0.699 &  0.268 &  0.978 &  0.005 &  0.999 &  0.000 &  0.914 &  0.024 \\\\\n",
      "adult        &  0.891 &  0.001 &  0.891 &  0.001 &  0.907 &  0.003 &  0.847 &  0.006 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stack = total_df.xs('train', level=2).stack()\n",
    "stack.index.names = ['metric','dataset',  'model']\n",
    "pivot = pd.pivot_table(pd.DataFrame(stack), index='dataset', columns = ['model','metric'])\n",
    "print(pivot.round(3).to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now the rest of the code considers getting correlation and cosine similarity measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for retrieving explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_explanations(results, X, y, pred_idx=0, printing=False):\n",
    "    \n",
    "    rho, slim_predictions, ebm, explainer, shap_values, logit = results   \n",
    "    \n",
    "    observation = pd.Series(1, index=['Constant']).append(X.iloc[pred_idx]).append(pd.Series(y.iloc[pred_idx], index=['prediction']))\n",
    "    \n",
    "    slim_contributions = X.iloc[pred_idx]*rho[1:]\n",
    "    slim_threshold = rho[0]\n",
    "    slim_prediction = slim_predictions.iloc[pred_idx]\n",
    "    slim_explanation = pd.Series(slim_threshold, index=['Constant']).append(slim_contributions).append(pd.Series(slim_prediction, index=['prediction'])) \n",
    "\n",
    "    logit_contributions = X.iloc[pred_idx]*logit.coef_.ravel()\n",
    "    logit_intercept = logit.intercept_\n",
    "    logit_prediction = int(sum(logit_contributions)+logit_intercept>0)\n",
    "    logit_explanation = pd.Series(logit_intercept, index=['Constant']).append(logit_contributions).append(pd.Series(logit_prediction, index=['prediction']))\n",
    "\n",
    "    ebm_local = ebm.explain_local(X.iloc[pred_idx:pred_idx+1], y.iloc[pred_idx:pred_idx+1], name='EBM')\n",
    "    ebm_contributions = pd.Series(ebm_local.data(0)['scores'], index=X_names[1:])\n",
    "    ebm_mean = ebm_local.data(0)['extra']['scores'][0]                               \n",
    "    ebm_prediction = int(sum(ebm_contributions)+ebm_mean>0)\n",
    "    ebm_explanation = pd.Series(ebm_mean, index= ['Constant']).append(ebm_contributions).append(pd.Series(ebm_prediction, index=['prediction']))\n",
    "\n",
    "    shap_contributions = pd.Series(shap_values[pred_idx,:], index=X_names[1:])\n",
    "    shap_mean = explainer.expected_value                              \n",
    "    shap_prediction =  int(sum(shap_contributions)+ shap_mean>0)\n",
    "    shap_explanation = pd.Series(shap_mean, index = ['Constant']).append(shap_contributions).append(pd.Series(shap_prediction, index=['prediction']))\n",
    "    \n",
    "    explanations = pd.DataFrame([observation, slim_explanation, logit_explanation, ebm_explanation, shap_explanation], index=['X', 'slim', 'logit', 'ebm', 'shap'])\n",
    "    explanations['sum'] = explanations[list(explanations.columns)].sum(axis=1)\n",
    "    if printing:\n",
    "        display(explanations)\n",
    "        print(\"prediction slim: \", slim_prediction)\n",
    "        print(\"prediction logit: \", logit_prediction)\n",
    "        print(\"prediction ebm: \", ebm_prediction)\n",
    "        print(\"prediction shap: \", shap_prediction)\n",
    "        print(\"true y: \", y.iloc[pred_idx])   \n",
    "    \n",
    "    return explanations\n",
    "\n",
    "def clean_prediction(df):\n",
    "    if df['prediction'].mean() in [0,1]: # unanimous\n",
    "        return df\n",
    "    \n",
    "def zero_filter(df):\n",
    "    sums = df.sum(axis=1)\n",
    "    if (sums.isin([0])*1).sum()==0:\n",
    "        return df\n",
    "\n",
    "def preprocess_results(expl, only_correct=True, no_zeros=True, printing=True):\n",
    "    # add observation index\n",
    "    complete_expl = pd.concat(expl, keys = [\"{:02d}\".format(x) for x in range(N)])\n",
    "    complete_expl.index.names = ['obs', 'model']\n",
    "    # clean to only include the contributions\n",
    "\n",
    "    if only_correct:\n",
    "        contrib = complete_expl.groupby(level='obs').apply(clean_prediction)\n",
    "        if len(contrib.index.names)>2:\n",
    "            contrib=contrib.droplevel(0)\n",
    "        contrib = contrib.drop(['X'], level = 1).drop(['Constant', 'sum', 'prediction'], axis=1)\n",
    "        contrib.index = contrib.index.remove_unused_levels()\n",
    "    else:    \n",
    "        contrib = complete_expl.drop(['X'], level = 1).drop(['Constant', 'sum', 'prediction'], axis=1)\n",
    "        contrib.index = contrib.index.remove_unused_levels()\n",
    "\n",
    "    if no_zeros:\n",
    "        contrib = contrib.groupby(level='obs').apply(zero_filter)\n",
    "        if len(contrib.index.names)>2:\n",
    "            contrib=contrib.droplevel(0)\n",
    "    \n",
    "    if printing:\n",
    "        print(\"Original number of explanations:  \", N)\n",
    "        print(\"Cleaned explanation set size:     \", int(contrib.shape[0]/4))\n",
    "    return contrib\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "def pearson(df):\n",
    "    return(pearsonr(df.iloc[0], df.iloc[1])[0])\n",
    "def get_pearson_matrix(df, func=pearson):\n",
    "    \n",
    "    models = list(df.index.levels[1])\n",
    "    p = len(models)\n",
    "    distance_matrix = np.zeros((p,p))\n",
    "    \n",
    "    for i in range(p):\n",
    "        for j in range(i+1,p):\n",
    "            to_drop = models.copy()\n",
    "            del to_drop[j]\n",
    "            del to_drop[i]\n",
    "            diff = df.drop(to_drop, level=1).groupby(level='obs').apply(func).mean()\n",
    "            distance_matrix[i,j] = round(diff,3)\n",
    "            distance_matrix[j,i] = round(diff,3)\n",
    "    distance_df = pd.DataFrame(distance_matrix, models, models)\n",
    "    distance_df['Average correlation'] = distance_df.sum()/(p-1)\n",
    "    return distance_df\n",
    "\n",
    "from scipy import spatial\n",
    "\n",
    "def cosine(df):\n",
    "    return(1-spatial.distance.cosine(df.iloc[0], df.iloc[1]))\n",
    "def get_cosine_matrix(df, func=cosine):\n",
    "    \n",
    "    models = list(df.index.levels[1])\n",
    "    p = len(models)\n",
    "    distance_matrix = np.zeros((p,p))\n",
    "    \n",
    "    for i in range(p):\n",
    "        for j in range(i+1,p):\n",
    "            to_drop = models.copy()\n",
    "            del to_drop[j]\n",
    "            del to_drop[i]\n",
    "            diff = df.drop(to_drop, level=1).groupby(level='obs').apply(func).mean()\n",
    "            distance_matrix[i,j] = round(diff,3)\n",
    "            distance_matrix[j,i] = round(diff,3)\n",
    "    distance_df = pd.DataFrame(distance_matrix, models, models)\n",
    "    distance_df['Average cosine similarity'] = distance_df.sum()/(p-1)\n",
    "    return distance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:  bankruptcy  fold:  0\n",
      "Original number of explanations:   51\n",
      "Cleaned explanation set size:      34\n",
      "Dataset:  bankruptcy  fold:  1\n",
      "Original number of explanations:   51\n",
      "Cleaned explanation set size:      48\n",
      "Dataset:  bankruptcy  fold:  2\n",
      "Original number of explanations:   50\n",
      "Cleaned explanation set size:      40\n",
      "Dataset:  bankruptcy  fold:  3\n",
      "Original number of explanations:   49\n",
      "Cleaned explanation set size:      43\n",
      "Dataset:  bankruptcy  fold:  4\n",
      "Original number of explanations:   49\n",
      "Cleaned explanation set size:      33\n",
      "Dataset:  breastcancer  fold:  0\n",
      "Original number of explanations:   137\n",
      "Cleaned explanation set size:      117\n",
      "Dataset:  breastcancer  fold:  1\n",
      "Original number of explanations:   137\n",
      "Cleaned explanation set size:      128\n",
      "Dataset:  breastcancer  fold:  2\n",
      "Original number of explanations:   137\n",
      "Cleaned explanation set size:      130\n",
      "Dataset:  breastcancer  fold:  3\n",
      "Original number of explanations:   137\n",
      "Cleaned explanation set size:      131\n",
      "Dataset:  breastcancer  fold:  4\n",
      "Original number of explanations:   135\n",
      "Cleaned explanation set size:      125\n",
      "Dataset:  haberman  fold:  0\n",
      "Original number of explanations:   62\n",
      "Cleaned explanation set size:      18\n",
      "Dataset:  haberman  fold:  1\n",
      "Original number of explanations:   61\n",
      "Cleaned explanation set size:      7\n",
      "Dataset:  haberman  fold:  2\n",
      "Original number of explanations:   61\n",
      "Cleaned explanation set size:      40\n",
      "Dataset:  haberman  fold:  3\n",
      "Original number of explanations:   61\n",
      "Cleaned explanation set size:      38\n",
      "Dataset:  haberman  fold:  4\n",
      "Original number of explanations:   61\n",
      "Cleaned explanation set size:      40\n",
      "Dataset:  heart  fold:  0\n",
      "Original number of explanations:   61\n",
      "Cleaned explanation set size:      38\n",
      "Dataset:  heart  fold:  1\n",
      "Original number of explanations:   61\n",
      "Cleaned explanation set size:      47\n",
      "Dataset:  heart  fold:  2\n",
      "Original number of explanations:   61\n",
      "Cleaned explanation set size:      35\n",
      "Dataset:  heart  fold:  3\n",
      "Original number of explanations:   61\n",
      "Cleaned explanation set size:      35\n",
      "Dataset:  heart  fold:  4\n",
      "Original number of explanations:   59\n",
      "Cleaned explanation set size:      37\n",
      "Dataset:  mammo  fold:  0\n",
      "Original number of explanations:   193\n",
      "Cleaned explanation set size:      76\n",
      "Dataset:  mammo  fold:  1\n",
      "Original number of explanations:   192\n",
      "Cleaned explanation set size:      71\n",
      "Dataset:  mammo  fold:  2\n",
      "Original number of explanations:   192\n",
      "Cleaned explanation set size:      72\n",
      "Dataset:  mammo  fold:  3\n",
      "Original number of explanations:   192\n",
      "Cleaned explanation set size:      63\n",
      "Dataset:  mammo  fold:  4\n",
      "Original number of explanations:   192\n",
      "Cleaned explanation set size:      125\n",
      "Dataset:  mushroom  fold:  0\n",
      "Original number of explanations:   1626\n",
      "Cleaned explanation set size:      351\n",
      "Dataset:  mushroom  fold:  1\n",
      "Original number of explanations:   1625\n",
      "Cleaned explanation set size:      842\n",
      "Dataset:  mushroom  fold:  2\n",
      "Original number of explanations:   1625\n",
      "Cleaned explanation set size:      842\n",
      "Dataset:  mushroom  fold:  3\n",
      "Original number of explanations:   1624\n",
      "Cleaned explanation set size:      841\n",
      "Dataset:  mushroom  fold:  4\n",
      "Original number of explanations:   1624\n",
      "Cleaned explanation set size:      306\n",
      "Dataset:  spambase  fold:  0\n",
      "Original number of explanations:   921\n",
      "Cleaned explanation set size:      302\n",
      "Dataset:  spambase  fold:  1\n",
      "Original number of explanations:   921\n",
      "Cleaned explanation set size:      498\n",
      "Dataset:  spambase  fold:  2\n",
      "Original number of explanations:   921\n",
      "Cleaned explanation set size:      307\n",
      "Dataset:  spambase  fold:  3\n",
      "Original number of explanations:   919\n",
      "Cleaned explanation set size:      149\n",
      "Dataset:  spambase  fold:  4\n",
      "Original number of explanations:   919\n",
      "Cleaned explanation set size:      126\n",
      "Dataset:  adult  fold:  0\n",
      "Original number of explanations:   6513\n",
      "Cleaned explanation set size:      4485\n",
      "Dataset:  adult  fold:  1\n",
      "Original number of explanations:   6512\n",
      "Cleaned explanation set size:      3513\n",
      "Dataset:  adult  fold:  2\n",
      "Original number of explanations:   6512\n",
      "Cleaned explanation set size:      4111\n",
      "Dataset:  adult  fold:  3\n",
      "Original number of explanations:   6512\n",
      "Cleaned explanation set size:      3741\n",
      "Dataset:  adult  fold:  4\n",
      "Original number of explanations:   6512\n",
      "Cleaned explanation set size:      3775\n"
     ]
    }
   ],
   "source": [
    "names = ['bankruptcy','breastcancer','haberman','heart','mammo','mushroom','spambase', 'adult']\n",
    "# names = ['mammo']\n",
    "correlations = []\n",
    "cosines = []\n",
    "folds = 5\n",
    "\n",
    "\n",
    "for dataname in names:\n",
    "    dataset_corrs = []\n",
    "    dataset_cosines = []\n",
    "    for fold in range(folds):\n",
    "        models = load_models(dataname+'_models_600_cv'+str(fold))\n",
    "        data = load_models(dataname+'_data_cv'+str(fold))\n",
    "        X_names = list(data['X_test'].columns.values)\n",
    "        X_names.insert(0, '(Intercept)')\n",
    "\n",
    "        N = data['X_test'].shape[0]\n",
    "        expl = []\n",
    "        results = extract_results(models, data['X_test'])\n",
    "        for idx in range(N):\n",
    "            expl.append(get_explanations(results,data['X_test'], data['y_test'], idx, printing=False))\n",
    "            \n",
    "        print(\"Dataset: \", dataname, \" fold: \", str(fold))\n",
    "        contrib = preprocess_results(expl, True, True, True)\n",
    "        \n",
    "        corr_distance = get_pearson_matrix(contrib)\n",
    "        dataset_corrs.append(corr_distance)\n",
    "        \n",
    "        cosine_distance = get_cosine_matrix(contrib)\n",
    "        dataset_cosines.append(cosine_distance)\n",
    "\n",
    "    dataset_corrs_df = pd.concat(dataset_corrs, keys = range(folds))\n",
    "    dataset_corrs_df.index.names = ['fold','model']\n",
    "    correlations.append(dataset_corrs_df.groupby('model').mean())\n",
    "    \n",
    "    dataset_cosines_df = pd.concat(dataset_cosines, keys = range(folds))\n",
    "    dataset_cosines_df.index.names = ['fold','model']\n",
    "    cosines.append(dataset_cosines_df.groupby('model').mean())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrr}\n",
      "\\toprule\n",
      "{} &   slim &  logit &    ebm &   shap \\\\\n",
      "model &        &        &        &        \\\\\n",
      "\\midrule\n",
      "slim  &  0.000 &  0.932 &  0.804 &  0.921 \\\\\n",
      "logit &  0.932 &  0.000 &  0.817 &  0.804 \\\\\n",
      "ebm   &  0.804 &  0.817 &  0.000 &  0.813 \\\\\n",
      "shap  &  0.921 &  0.804 &  0.813 &  0.000 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\\begin{tabular}{lrrrr}\n",
      "\\toprule\n",
      "{} &   slim &  logit &    ebm &   shap \\\\\n",
      "model &        &        &        &        \\\\\n",
      "\\midrule\n",
      "slim  &  0.000 &  0.935 &  0.826 &  0.929 \\\\\n",
      "logit &  0.935 &  0.000 &  0.841 &  0.834 \\\\\n",
      "ebm   &  0.826 &  0.841 &  0.000 &  0.861 \\\\\n",
      "shap  &  0.929 &  0.834 &  0.861 &  0.000 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = 0\n",
    "print(correlations[dataset].drop(columns=['Average correlation'])[['slim','logit','ebm', 'shap']].round(3).reindex(['slim', 'logit', 'ebm', 'shap']).to_latex())\n",
    "print(cosines[dataset].drop(columns=['Average cosine similarity'])[['slim','logit','ebm', 'shap']].round(3).reindex(['slim', 'logit', 'ebm', 'shap']).to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations_df = pd.concat(correlations, keys = names)\n",
    "correlations_df.index.names = ['dataset','model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrr}\n",
      "\\toprule\n",
      "{} &   slim &  logit &    ebm &   shap \\\\\n",
      "model &        &        &        &        \\\\\n",
      "\\midrule\n",
      "slim  &  0.000 &  0.697 &  0.383 &  0.505 \\\\\n",
      "logit &  0.697 &  0.000 &  0.450 &  0.511 \\\\\n",
      "ebm   &  0.383 &  0.450 &  0.000 &  0.622 \\\\\n",
      "shap  &  0.505 &  0.511 &  0.622 &  0.000 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cor_avgs = correlations_df.groupby('model').mean()\n",
    "print(cor_avgs.drop(columns=['Average correlation'])[['slim','logit','ebm', 'shap']].round(3).reindex(['slim', 'logit', 'ebm', 'shap']).to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrr}\n",
      "\\toprule\n",
      "{} &   slim &  logit &    ebm &   shap \\\\\n",
      "model &        &        &        &        \\\\\n",
      "\\midrule\n",
      "slim  &  0.000 &  0.250 &  0.300 &  0.294 \\\\\n",
      "logit &  0.250 &  0.000 &  0.273 &  0.198 \\\\\n",
      "ebm   &  0.300 &  0.273 &  0.000 &  0.289 \\\\\n",
      "shap  &  0.294 &  0.198 &  0.289 &  0.000 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cor_stds = correlations_df.groupby('model').std()\n",
    "print(cor_stds.drop(columns=['Average correlation'])[['slim','logit','ebm', 'shap']].round(3).reindex(['slim', 'logit', 'ebm', 'shap']).to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosines_df = pd.concat(cosines, keys = names)\n",
    "cosines_df.index.names = ['dataset','model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrr}\n",
      "\\toprule\n",
      "{} &   slim &  logit &    ebm &   shap \\\\\n",
      "model &        &        &        &        \\\\\n",
      "\\midrule\n",
      "slim  &  0.000 &  0.707 &  0.326 &  0.410 \\\\\n",
      "logit &  0.707 &  0.000 &  0.416 &  0.492 \\\\\n",
      "ebm   &  0.326 &  0.416 &  0.000 &  0.657 \\\\\n",
      "shap  &  0.410 &  0.492 &  0.657 &  0.000 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cos_avgs = cosines_df.groupby('model').mean()\n",
    "print(cos_avgs.drop(columns=['Average cosine similarity'])[['slim','logit','ebm', 'shap']].round(3).reindex(['slim', 'logit', 'ebm', 'shap']).to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrr}\n",
      "\\toprule\n",
      "{} &   slim &  logit &    ebm &   shap \\\\\n",
      "model &        &        &        &        \\\\\n",
      "\\midrule\n",
      "slim  &  0.000 &  0.239 &  0.325 &  0.375 \\\\\n",
      "logit &  0.239 &  0.000 &  0.361 &  0.369 \\\\\n",
      "ebm   &  0.325 &  0.361 &  0.000 &  0.308 \\\\\n",
      "shap  &  0.375 &  0.369 &  0.308 &  0.000 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cos_stds = cosines_df.groupby('model').std()\n",
    "print(cos_stds.drop(columns=['Average cosine similarity'])[['slim','logit','ebm', 'shap']].round(3).reindex(['slim', 'logit', 'ebm', 'shap']).to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
